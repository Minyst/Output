{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpohurPU1Ccp",
    "outputId": "c215d852-2549-4a47-b8ae-07a17509c759"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\user\\anaconda3\\lib\\site-packages (1.63.2)\n",
      "Requirement already satisfied: gradio in c:\\users\\user\\anaconda3\\lib\\site-packages (5.17.0)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\user\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\user\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.115.8)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (1.7.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.29.1)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (3.10.15)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.9.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio) (0.34.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio-client==1.7.1->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<15.0,>=10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai gradio tiktoken faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SMTDrE9x1q-I"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] =  \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jGHfqZsGnIEC"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Type, List\n",
    "from sentence_transformers import CrossEncoder\n",
    "import gradio as gr\n",
    "import json\n",
    "import time\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "K_fxbHVmhC6J"
   },
   "outputs": [],
   "source": [
    "# ✅ LLM 생성\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, model_name=\"gpt-4o-mini\", temperature=0.1, max_tokens=2048):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.chat_history = []  # ✅ LLM은 대화 기록만 관리\n",
    "\n",
    "    def add_to_memory(self, role, content):\n",
    "        \"\"\"대화 기록 저장\"\"\"\n",
    "        self.chat_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def generate_response(self, user_input, add_to_history=True):\n",
    "        \"\"\"LLM을 호출하여 응답 생성\"\"\"\n",
    "        if add_to_history:\n",
    "            self.add_to_memory(\"user\", user_input) # ✅ 입력 저장\n",
    "\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=self.chat_history,  # ✅ 대화 기록만 전달\n",
    "            temperature=self.temperature,\n",
    "            max_tokens=self.max_tokens\n",
    "        )\n",
    "\n",
    "        ai_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        if add_to_history:\n",
    "            self.add_to_memory(\"assistant\", ai_message) # ✅ 출력 저장\n",
    "\n",
    "        return ai_message\n",
    "\n",
    "# ✅ LLM 인스턴스 생성\n",
    "llm = LLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QIjIy9Oqfc8z"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "r1t0JLs0hHg5"
   },
   "outputs": [],
   "source": [
    "# ✅ 크롤링\n",
    "\n",
    "service = Service(\"C:/Users/USER/Desktop/chromedriver-win64/chromedriver-win64/chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.implicitly_wait(2)\n",
    "search_page = 4\n",
    "sort = 1  # 최신순\n",
    "keywords = [\"프리미어리그\", \"프리메라리가\", \"분데스리가\", \"세리에A\", \"리그앙\", \"챔피언스리그\", \"fa컵\"]\n",
    "data_list = []\n",
    "\n",
    "try:\n",
    "    for keyword in keywords:\n",
    "        for i in range(search_page):\n",
    "            start_num = 1 if i == 0 else (i * 10) + 1\n",
    "            url = f'https://search.naver.com/search.naver?where=news&query={keyword}&sort={sort}&start={start_num}'\n",
    "\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            articles = soup.select('ul.list_news > li')\n",
    "            for article in articles:\n",
    "                media_name = article.select_one('.info.press').text if article.select_one('.info.press') else \"Unknown\"\n",
    "                title = article.select_one('.news_tit').text if article.select_one('.news_tit') else \"Unknown\"\n",
    "                content = article.select_one('.dsc_wrap').text if article.select_one('.dsc_wrap') else \"Unknown\"\n",
    "                data_list.append({\"media_name\": media_name, \"title\": title, \"content\": content})\n",
    "\n",
    "    with open(\"output.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        crawled_json = json.dump(data_list, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0u1dxzk4zEhp"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import openai\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# ✅ RecursiveTextSplitter 구현하기\n",
    "\n",
    "def split_text(text, chunk_size, chunk_overlap):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "# ✅ OpenAI 임베딩 생성 함수\n",
    "\n",
    "def get_openai_embedding(texts):\n",
    "    client = openai.OpenAI()\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=\"text-embedding-3-small\"  \n",
    "    )\n",
    "    return np.array([res.embedding for res in response.data])  \n",
    "\n",
    "# ✅ 문서 로드 함수\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        return data\n",
    "data = load_data(\"C:/Users/USER/Documents/output.json\")\n",
    "\n",
    "# ✅ 문서 분할 및 임베딩 생성\n",
    "\n",
    "def prepare_documents(data):\n",
    "    chunked_data = []\n",
    "    chunk_to_original = {}\n",
    "    all_chunks = []\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        media_chunks = split_text(item[\"media_name\"], chunk_size=50, chunk_overlap=5)\n",
    "        title_chunks = split_text(item[\"title\"], chunk_size=200, chunk_overlap=5)\n",
    "        content_chunks = split_text(item[\"content\"], chunk_size=200, chunk_overlap=5)\n",
    "\n",
    "        for j, (media_chunk, title_chunk, content_chunk) in enumerate(zip(media_chunks, title_chunks, content_chunks)):\n",
    "            chunk_id = f\"{i}-{j}\"\n",
    "            chunk_text = media_chunk + \" \" + title_chunk + \" \" + content_chunk\n",
    "            chunked_data.append({\n",
    "                \"id\": chunk_id,\n",
    "                \"media_name\": media_chunk,\n",
    "                \"title\": title_chunk,\n",
    "                \"content\": content_chunk\n",
    "            })\n",
    "            chunk_to_original[chunk_id] = i\n",
    "            all_chunks.append(chunk_text)  # ✅ 한 번에 임베딩할 리스트에 저장\n",
    "\n",
    "    embeddings = get_openai_embedding(all_chunks) # ✅ 한 번에 모든 임베딩 처리\n",
    "    \n",
    "    return chunked_data, chunk_to_original, np.array(embeddings)\n",
    "    \n",
    "chunked_data, chunk_to_original, embeddings = prepare_documents(data)  \n",
    "\n",
    "# ✅ FAISS 인덱스 생성\n",
    "\n",
    "def build_faiss_retriever(embeddings):\n",
    "    d = embeddings.shape[1]  # 벡터 차원\n",
    "    index = faiss.IndexFlatL2(d)  # L2 거리 기반 인덱스 생성\n",
    "    index.add(embeddings)  # FAISS 인덱스에 벡터 추가\n",
    "    return index\n",
    "\n",
    "index = build_faiss_retriever(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Function Calling용 빠른 검색\n",
    "\n",
    "def function_calling_search_documents(query, index, chunked_data, k=5):\n",
    "    \"\"\"FAISS를 이용한 빠른 문서 검색 (Function Calling 전용, Reranking 없음)\"\"\"\n",
    "    # 1. query를 임베딩 벡터로 변환합니다.\n",
    "    query_embedding = get_openai_embedding(query)\n",
    "    \n",
    "    # 2. FAISS 인덱스에서 k개의 유사 문서를 검색합니다.\n",
    "    # 원래는 D, I = index.search(...)로 두 값을 받도록 되어 있었는데,\n",
    "    # 실제 반환값이 2개보다 많으면 unpack 에러가 발생하므로, 첫 두 값만 사용하고 나머지는 무시합니다.\n",
    "    D, I, *_ = index.search(np.array([query_embedding]), k=k)\n",
    "    \n",
    "    # 3. 검색 결과 인덱스(I[0])를 사용해 chunked_data에서 해당 문서를 추출하여 반환합니다.\n",
    "    return [chunked_data[idx] for idx in I[0] if 0 <= idx < len(chunked_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 검색 및 Reranking\n",
    "\n",
    "def rag_search_documents(query, index, chunked_data, k=5, rerank_k=2):\n",
    "    \"\"\"FAISS를 이용해 문서 검색 후 Reranking을 수행하는 함수\"\"\"\n",
    "    query_embedding = get_openai_embedding(query)\n",
    "    D, I = index.search(np.array([query_embedding]), k=k)\n",
    "    \n",
    "    retrieved_chunks = [chunked_data[idx] for idx in I[0] if 0 <= idx < len(chunked_data)]\n",
    "    if not retrieved_chunks:\n",
    "        return []\n",
    "    \n",
    "    reranker_model = CrossEncoder('BAAI/bge-reranker-v2-m3')\n",
    "    rerank_inputs = [(query, f\"{chunk['media_name']} {chunk['title']} {chunk['content']}\") for chunk in retrieved_chunks]\n",
    "    rerank_scores = reranker_model.predict(rerank_inputs)\n",
    "    \n",
    "    reranked_results = sorted(zip(retrieved_chunks, rerank_scores), key=lambda x: x[1], reverse=True)[:rerank_k]\n",
    "    return [result[0] for result in reranked_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xzQ4lx474kir"
   },
   "outputs": [],
   "source": [
    "# ✅ Function Calling Agent\n",
    "\n",
    "class FunctionCallingAgent:\n",
    "    def __init__(self, llm, index, chunked_data):\n",
    "        self.llm = llm\n",
    "        self.index = index\n",
    "        self.chunked_data = chunked_data\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def get_best_documents(self, query):\n",
    "        \"\"\" 빠른 검색을 수행하는 함수 (Reranking 없음) \"\"\"\n",
    "        return function_calling_search_documents(query, self.index, self.chunked_data)\n",
    "    \n",
    "    def call_function(self, function_name, args):\n",
    "        if function_name == \"get_best_documents\":\n",
    "            return json.dumps(self.get_best_documents(**args), ensure_ascii=False, indent=4)\n",
    "        return None\n",
    "    \n",
    "    def generate_response(self, user_input):\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        retrieved_docs = self.get_best_documents(user_input)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        당신은 사용자 질문에 대한 정보를 제공하는 챗봇입니다.\n",
    "\n",
    "        참고 문서:\n",
    "        {json.dumps(retrieved_docs, ensure_ascii=False, indent=4)}\n",
    "\n",
    "        질문: {user_input}\n",
    "        답변:\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.llm.model_name,\n",
    "            messages=messages,\n",
    "            temperature=self.llm.temperature,\n",
    "            max_tokens=self.llm.max_tokens\n",
    "        )\n",
    "        \n",
    "        ai_message = response[\"choices\"][0][\"message\"]\n",
    "        self.chat_history.append(ai_message)\n",
    "        return ai_message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uMnc9LK27AVu"
   },
   "outputs": [],
   "source": [
    "# ✅ RAG Agent\n",
    "\n",
    "class RAGAgent:\n",
    "    def __init__(self, llm, index, chunked_data):\n",
    "        self.llm = llm\n",
    "        self.index = index\n",
    "        self.chunked_data = chunked_data\n",
    "        self.chat_history = []\n",
    "    \n",
    "    def retriever(self, query):\n",
    "        return rag_search_documents(query, self.index, self.chunked_data)\n",
    "    \n",
    "    def generate_response(self, user_input):\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        retrieved_docs = self.retriever(user_input)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        {fixed_context}\n",
    "\n",
    "        참고 문서:\n",
    "        {json.dumps(retrieved_docs, ensure_ascii=False, indent=4)}\n",
    "\n",
    "        질문: {user_input}\n",
    "        답변:\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"system\", \"content\": prompt}]\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.llm.model_name,\n",
    "            messages=messages,\n",
    "            temperature=self.llm.temperature,\n",
    "            max_tokens=self.llm.max_tokens\n",
    "        )\n",
    "        \n",
    "        ai_message = response[\"choices\"][0][\"message\"]\n",
    "        self.chat_history.append(ai_message)\n",
    "        return ai_message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Fuction Calling & RAG 인스턴스 만들기\n",
    "\n",
    "function_agent = FunctionCallingAgent(llm=llm, index=index, chunked_data=chunked_data)\n",
    "rag_agent = RAGAgent(llm=llm, index=index, chunked_data=chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M2k54n_t7iNr"
   },
   "outputs": [],
   "source": [
    "# ✅ Function Calling 기반 LLM 응답 생성\n",
    "\n",
    "def function_calling_prompt_agent(input_query):\n",
    "    return function_agent.generate_response(input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ze17eTzx7XgS"
   },
   "outputs": [],
   "source": [
    "# ✅ RAG 기반 LLM 응답 생성\n",
    "\n",
    "def rag_prompt_agent(input_query):\n",
    "    return rag_agent.generate_response(input_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PUkmcJyS7njF"
   },
   "outputs": [],
   "source": [
    "# ✅사용자의 질문을 받고 Function Calling + RAG 기반으로 응답 생성\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    # Function Calling (웹 스크래핑 기반)\n",
    "    try:\n",
    "        result = function_calling_prompt_agent(message)  # ✅ Function Calling 실행\n",
    "    except Exception as e:\n",
    "        print(f\"Function Calling 실패: {e}\")\n",
    "        result = None\n",
    "\n",
    "    # Function Calling 실패 시 RAG 검색 실행\n",
    "    if not result or \"관련 정보 없음\" in result:\n",
    "        print(\"Function Calling에서 정보를 찾지 못함. RAG 실행.\")\n",
    "        result = rag_prompt_agent(message)  # ✅ RAG 실행\n",
    "\n",
    "    # 챗봇 대화 기록에 추가\n",
    "    chat_history.append((message, result))\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QwU8AB7-lg5"
   },
   "outputs": [],
   "source": [
    "# ✅ Gradio\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"축구 정보 챗봇\")\n",
    "    msg = gr.Textbox(label=\"질문해주세요!\")\n",
    "    clear = gr.Button(\"대화 초기화\")\n",
    "\n",
    "    msg.submit(respond, [msg, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
